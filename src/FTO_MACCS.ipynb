{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e411faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycaret.classification import *\n",
    "import os\n",
    "import joblib\n",
    "import shutil\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MACCSkeys\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='lightgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7ce6d",
   "metadata": {},
   "source": [
    "# 세팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af626c1",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6ac715",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('../data/preprocessed/filtered_FTO_training_total_ignore3D_False.csv')\n",
    "raw = raw[[col for col in raw.columns if not col.startswith('X')]]\n",
    "raw['source'].value_counts()\n",
    "\n",
    "def get_maccs(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return [None] * 167\n",
    "    return list(MACCSkeys.GenMACCSKeys(mol))\n",
    "\n",
    "maccs_df = pd.DataFrame(\n",
    "    raw['canonical_SMILES'].apply(get_maccs).tolist(),\n",
    "    columns=[f'X{i}' for i in range(167)]\n",
    ")\n",
    "\n",
    "maccs_md = pd.concat([raw, maccs_df], axis=1)\n",
    "\n",
    "active_df = maccs_md[maccs_md['source'].isin(['active', 'assay_inactive'])]\n",
    "decoy_df = maccs_md[maccs_md['source'] == 'decoy']\n",
    "\n",
    "n_active = len(active_df)\n",
    "decoy_5x = decoy_df.sample(n=n_active * 5, random_state=42)\n",
    "decoy_10x = decoy_df.sample(n=n_active * 10, random_state=42)\n",
    "\n",
    "maccs_5x = pd.concat([active_df, decoy_5x]).reset_index(drop=True)\n",
    "maccs_10x = pd.concat([active_df, decoy_10x]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b95986",
   "metadata": {},
   "source": [
    "## 사용할 descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d04a3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_descriptor = pd.read_csv('../data/descriptor_selection.csv')\n",
    "\n",
    "file_md_list = {}\n",
    "for column in selected_descriptor.columns:\n",
    "    filename = column\n",
    "    selected_columns = selected_descriptor[column].iloc[0:].dropna().tolist()\n",
    "    if filename and selected_columns:\n",
    "        file_md_list[filename] = selected_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a81072",
   "metadata": {},
   "source": [
    "## 저장 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75bd54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_move_file(src, dst):\n",
    "    if os.path.exists(src):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "            shutil.move(src, dst)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"파일 이동 실패: {src} -> {dst}, 에러: {e}\")\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def safe_save_csv(df, path):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 저장 실패: {path}, 에러: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1521974",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_filenames = {\n",
    "    'auc': 'AUC.png',\n",
    "    'confusion_matrix': 'Confusion Matrix.png',\n",
    "    'learning': 'Learning Curve.png',\n",
    "    'feature': 'Feature Importance.png',\n",
    "    'error': 'Prediction Error.png',\n",
    "    'calibration': 'Calibration Curve.png'\n",
    "}\n",
    "\n",
    "target_models = ['lr', 'et', 'gbc', 'lightgbm', 'svm', 'rf', 'ada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba61c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"..\", \"data\", \"preprocessed\")\n",
    "result_dir = os.path.join(\"..\", \"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60bf1e1",
   "metadata": {},
   "source": [
    "# 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c3b061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================FTO_Final/5x_w3D start==================================================\n",
      "Creating & tuning lr...\n",
      "lr (LogisticRegression) 저장 완료 - F1: 0.6514\n",
      "Creating & tuning et...\n",
      "et (ExtraTreesClassifier) 저장 완료 - F1: 0.6982\n",
      "Creating & tuning gbc...\n",
      "gbc (GradientBoostingClassifier) 저장 완료 - F1: 0.7171\n",
      "Creating & tuning lightgbm...\n",
      "lightgbm (LGBMClassifier) 저장 완료 - F1: 0.7144\n",
      "Creating & tuning svm...\n",
      "svm (SGDClassifier) 저장 완료 - F1: 0.6426\n",
      "Creating & tuning rf...\n",
      "rf (RandomForestClassifier) 저장 완료 - F1: 0.6844\n",
      "Creating & tuning ada...\n",
      "ada (AdaBoostClassifier) 저장 완료 - F1: 0.6515\n",
      "\n",
      "[5x] 블렌딩 선정 모델 (F1 기준 상위 4개):\n",
      "  gbc: 0.7171\n",
      "  lightgbm: 0.7144\n",
      "  et: 0.6982\n",
      "  rf: 0.6844\n",
      "Blending 5x...\n",
      "최종 예측 완료\n",
      "\n",
      "SHAP - Best 모델: ExtraTreesClassifier (AUC: 0.8387)\n",
      "TreeExplainer 사용\n",
      "SHAP 저장 완료 -> ..\\result\\FTO_Final/5x_w3D\\SHAP\n",
      "==================================================FTO_Final/5x_w3D completed==================================================\n",
      "==================================================FTO_Final/10x_w3D start==================================================\n",
      "Creating & tuning lr...\n",
      "lr (LogisticRegression) 저장 완료 - F1: 0.6141\n",
      "Creating & tuning et...\n",
      "et (ExtraTreesClassifier) 저장 완료 - F1: 0.6806\n",
      "Creating & tuning gbc...\n",
      "gbc (GradientBoostingClassifier) 저장 완료 - F1: 0.7018\n",
      "Creating & tuning lightgbm...\n",
      "lightgbm (LGBMClassifier) 저장 완료 - F1: 0.7021\n",
      "Creating & tuning svm...\n",
      "svm (SGDClassifier) 저장 완료 - F1: 0.6087\n",
      "Creating & tuning rf...\n",
      "rf (RandomForestClassifier) 저장 완료 - F1: 0.6649\n",
      "Creating & tuning ada...\n",
      "ada (AdaBoostClassifier) 저장 완료 - F1: 0.6302\n",
      "\n",
      "[10x] 블렌딩 선정 모델 (F1 기준 상위 4개):\n",
      "  lightgbm: 0.7021\n",
      "  gbc: 0.7018\n",
      "  et: 0.6806\n",
      "  rf: 0.6649\n",
      "Blending 10x...\n",
      "최종 예측 완료\n",
      "\n",
      "SHAP - Best 모델: ExtraTreesClassifier (AUC: 0.8405)\n",
      "TreeExplainer 사용\n",
      "SHAP 저장 완료 -> ..\\result\\FTO_Final/10x_w3D\\SHAP\n",
      "==================================================FTO_Final/10x_w3D completed==================================================\n",
      "\n",
      "전체 결과 요약:\n",
      "\n",
      "[5x]\n",
      "  블렌딩 모델: gbc, lightgbm, et, rf\n",
      "  블렌드 평균 F1: 0.6507\n",
      "  SHAP Best 모델: ExtraTreesClassifier\n",
      "  개별 모델 F1:\n",
      "    gbc: 0.7171\n",
      "    lightgbm: 0.7144\n",
      "    et: 0.6982\n",
      "    rf: 0.6844\n",
      "\n",
      "[10x]\n",
      "  블렌딩 모델: lightgbm, gbc, et, rf\n",
      "  블렌드 평균 F1: 0.6381\n",
      "  SHAP Best 모델: ExtraTreesClassifier\n",
      "  개별 모델 F1:\n",
      "    lightgbm: 0.7021\n",
      "    gbc: 0.7018\n",
      "    et: 0.6806\n",
      "    rf: 0.6649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_blended_results = {}\n",
    "\n",
    "for ratio in ['5x', '10x']:\n",
    "    file_name = f'descriptors_filtered_FTO_training_{ratio}_ignore3D_False.csv'\n",
    "    base_path = f'FTO_MACCS/{ratio}_w3D'\n",
    "\n",
    "    if ratio == '5x':\n",
    "        df = maccs_5x\n",
    "    else:\n",
    "        df = maccs_10x\n",
    "    print(f\"{'='*50}{base_path} start{'='*50}\")\n",
    "\n",
    "    full_result_path = os.path.join(result_dir, base_path)\n",
    "    models_dir      = os.path.join(full_result_path, \"models\")\n",
    "    plots_dir       = os.path.join(full_result_path, \"plots\")\n",
    "    blend_models_dir = os.path.join(full_result_path, \"blend_models\")\n",
    "    blend_plots_dir  = os.path.join(full_result_path, \"blend_plots\")\n",
    "    shap_dir        = os.path.join(full_result_path, \"SHAP\")\n",
    "\n",
    "    for dir_path in [full_result_path, models_dir, plots_dir, blend_models_dir, blend_plots_dir, shap_dir]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    md_cols = file_md_list[file_name]\n",
    "    fp_cols = [f'X{i+1}' for i in range(166)]\n",
    "    filtered_df = df[['potency'] + fp_cols + md_cols]\n",
    "\n",
    "    X = filtered_df.drop('potency', axis=1)\n",
    "    Y = filtered_df['potency']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42, stratify=Y)\n",
    "    df_train = pd.concat([x_train, y_train], axis=1)\n",
    "\n",
    "    setup(\n",
    "        data=df_train,\n",
    "        target='potency',\n",
    "        session_id=42,\n",
    "        train_size=0.9,\n",
    "        fold=10,\n",
    "        normalize=True,\n",
    "        fix_imbalance=True,\n",
    "        remove_outliers=True,\n",
    "        n_jobs=1,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # 전처리 파이프라인 (SHAP용)\n",
    "    pipeline = get_config('pipeline')\n",
    "    X_train_transformed = pipeline.transform(x_train)\n",
    "    X_test_transformed  = pipeline.transform(x_test)\n",
    "\n",
    "    if hasattr(X_test_transformed, 'columns'):\n",
    "        feature_names = X_test_transformed.columns.tolist()\n",
    "        X_train_df = X_train_transformed\n",
    "        X_test_df  = X_test_transformed\n",
    "    else:\n",
    "        feature_names = [f'f{i}' for i in range(X_test_transformed.shape[1])]\n",
    "        X_train_df = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
    "        X_test_df  = pd.DataFrame(X_test_transformed,  columns=feature_names)\n",
    "\n",
    "    # ── 개별 모델 학습 ──────────────────────────────────────\n",
    "    summary_data = []\n",
    "    trained_models = {}\n",
    "    individual_results = {}\n",
    "\n",
    "    for model_id in target_models:\n",
    "        print(f\"Creating & tuning {model_id}...\")\n",
    "        if model_id == 'lightgbm':\n",
    "            lgb_params = {\n",
    "                'boosting_type': 'gbdt', 'num_leaves': 31, 'learning_rate': 0.1,\n",
    "                'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5,\n",
    "                'min_child_samples': 20, 'random_state': 42, 'n_estimators': 100, 'verbosity': -1\n",
    "            }\n",
    "            model = create_model(model_id, verbose=False, **lgb_params)\n",
    "        else:\n",
    "            model = create_model(model_id, verbose=False)\n",
    "\n",
    "        tuned_model = tune_model(\n",
    "            model, optimize='F1', n_iter=50, fold=5, choose_better=True, verbose=False\n",
    "        )\n",
    "        results = pull()\n",
    "        model_name = tuned_model.__class__.__name__\n",
    "\n",
    "        trained_models[model_id] = tuned_model\n",
    "        individual_results[model_id] = results\n",
    "\n",
    "        joblib.dump(tuned_model, os.path.join(models_dir, f\"{ratio}_{model_name}_model.pkl\"))\n",
    "        safe_save_csv(results, os.path.join(full_result_path, f\"{ratio}_{model_name}_evaluation.csv\"))\n",
    "        safe_save_csv(results, os.path.join(full_result_path, f\"{ratio}_{model_id}_individual_results.csv\"))\n",
    "        print(f\"{model_id} ({model_name}) 저장 완료 - F1: {results['F1'].mean():.4f}\")\n",
    "\n",
    "        for plot_type, default_name in default_filenames.items():\n",
    "            try:\n",
    "                plot_model(tuned_model, plot=plot_type, save=True, verbose=False)\n",
    "                if os.path.exists(default_name):\n",
    "                    shutil.move(default_name, os.path.join(plots_dir, f\"{ratio}_{model_name}_{plot_type}.png\"))\n",
    "            except Exception as e:\n",
    "                with open(os.path.join(plots_dir, f\"{ratio}_{model_name}_{plot_type}_error.txt\"), 'w') as f:\n",
    "                    f.write(f\"Plot Type: {plot_type}\\nModel: {model_name}\\nError: {str(e)}\\n\")\n",
    "\n",
    "        numeric_cols = results.select_dtypes(include=[np.number]).columns\n",
    "        avg_row = results[numeric_cols].mean().to_dict()\n",
    "        std_row = results[numeric_cols].std().to_dict()\n",
    "        avg_row.update({'Model': model_name, 'Type': 'Mean'})\n",
    "        std_row.update({'Model': model_name, 'Type': 'Std'})\n",
    "        summary_data.extend([avg_row, std_row])\n",
    "\n",
    "    if summary_data:\n",
    "        combined_summary = pd.DataFrame(summary_data)\n",
    "        safe_save_csv(combined_summary, os.path.join(full_result_path, f\"{ratio}_summary_evaluation.csv\"))\n",
    "\n",
    "    # ── F1 기준 상위 4개 모델 선정 ─────────────────────────\n",
    "    model_f1_scores = {mid: individual_results[mid]['F1'].mean() for mid in individual_results}\n",
    "    top4_model_ids  = sorted(model_f1_scores, key=model_f1_scores.get, reverse=True)[:4]\n",
    "    print(f\"\\n[{ratio}] 블렌딩 선정 모델 (F1 기준 상위 4개):\")\n",
    "    for mid in top4_model_ids:\n",
    "        print(f\"  {mid}: {model_f1_scores[mid]:.4f}\")\n",
    "\n",
    "    fresh_models = [trained_models[mid] for mid in top4_model_ids]\n",
    "    model_names  = [trained_models[mid].__class__.__name__ for mid in top4_model_ids]\n",
    "\n",
    "    # ── 블렌딩 ─────────────────────────────────────────────\n",
    "    print(f\"Blending {ratio}...\")\n",
    "    blended_model = blend_models(\n",
    "        estimator_list=fresh_models, verbose=False, fold=3, method='soft'\n",
    "    )\n",
    "    blend_results = pull()\n",
    "\n",
    "    joblib.dump(blended_model, os.path.join(blend_models_dir, f\"{ratio}_blended_model2.pkl\"))\n",
    "    safe_save_csv(blend_results, os.path.join(full_result_path, f\"{ratio}_blend_evaluation.csv\"))\n",
    "\n",
    "    test_data = pd.concat([x_test, y_test], axis=1)\n",
    "    try:\n",
    "        final_predictions = predict_model(blended_model, data=test_data, verbose=False)\n",
    "        final_metrics = pull()\n",
    "        safe_save_csv(final_metrics, os.path.join(full_result_path, f\"{ratio}_blend_final_metrics.csv\"))\n",
    "        print(\"최종 예측 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"Final prediction failed: {e}\")\n",
    "        final_metrics = None\n",
    "\n",
    "    for plot_type in ['auc', 'confusion_matrix', 'learning', 'feature']:\n",
    "        try:\n",
    "            plot_model(blended_model, plot=plot_type, save=True, verbose=False)\n",
    "            default_name = default_filenames.get(plot_type, f'{plot_type}.png')\n",
    "            if os.path.exists(default_name):\n",
    "                safe_move_file(default_name, os.path.join(blend_plots_dir, f\"{ratio}_blend_{plot_type}.png\"))\n",
    "        except Exception as e:\n",
    "            with open(os.path.join(blend_plots_dir, f\"{ratio}_blend_{plot_type}_error.txt\"), 'w') as f:\n",
    "                f.write(f\"Plot Type: {plot_type}\\nModel: Blended\\nError: {str(e)}\\n\")\n",
    "\n",
    "    # ── SHAP (AUC 기준 Best 모델) ───────────────────────────\n",
    "    mean_df  = combined_summary[combined_summary['Type'] == 'Mean']\n",
    "    best_row = mean_df.loc[mean_df['AUC'].idxmax()]\n",
    "    best_model_name = best_row['Model']\n",
    "    best_model_id   = [mid for mid, m in trained_models.items()\n",
    "                       if m.__class__.__name__ == best_model_name][0]\n",
    "    best_model = trained_models[best_model_id]\n",
    "    print(f\"\\nSHAP - Best 모델: {best_model_name} (AUC: {best_row['AUC']:.4f})\")\n",
    "\n",
    "    if 'LogisticRegression' in best_model_name:\n",
    "        explainer = shap.LinearExplainer(\n",
    "            best_model, X_train_df, feature_perturbation=\"correlation_dependent\"\n",
    "        )\n",
    "        shap_values_class1 = explainer.shap_values(X_test_df)\n",
    "        print(\"LinearExplainer 사용\")\n",
    "    else:\n",
    "        try:\n",
    "            explainer = shap.TreeExplainer(best_model)\n",
    "            shap_values = explainer.shap_values(X_test_df)\n",
    "            shap_values_class1 = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
    "            print(\"TreeExplainer 사용\")\n",
    "        except Exception as e:\n",
    "            print(f\"TreeExplainer 실패: {e} -> KernelExplainer 사용\")\n",
    "            X_background = shap.sample(X_train_df, 50)\n",
    "            explainer = shap.KernelExplainer(best_model.predict_proba, X_background)\n",
    "            shap_values = explainer.shap_values(X_test_df, nsamples=100)\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values_class1 = shap_values[1]\n",
    "            elif isinstance(shap_values, np.ndarray) and shap_values.ndim == 3:\n",
    "                shap_values_class1 = shap_values[:, :, 1]\n",
    "            else:\n",
    "                shap_values_class1 = shap_values\n",
    "            print(\"KernelExplainer 사용\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values_class1, X_test_df, feature_names=feature_names, show=False, max_display=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(shap_dir, f'shap_{best_model_name}_summary.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_class1, X_test_df, feature_names=feature_names,\n",
    "                      plot_type='bar', show=False, max_display=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(shap_dir, f'shap_{best_model_name}_bar.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    mean_abs_shap = np.abs(shap_values_class1).mean(axis=0)\n",
    "    if mean_abs_shap.ndim > 1:\n",
    "        mean_abs_shap = mean_abs_shap.mean(axis=-1)\n",
    "    mean_abs_shap = mean_abs_shap.flatten()\n",
    "    pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'mean_abs_shap': mean_abs_shap\n",
    "    }).sort_values('mean_abs_shap', ascending=False).to_csv(\n",
    "        os.path.join(shap_dir, f'shap_{best_model_name}_importance.csv'), index=False\n",
    "    )\n",
    "    print(f\"SHAP 저장 완료 -> {shap_dir}\")\n",
    "\n",
    "    all_blended_results[ratio] = {\n",
    "        'blend_results': blend_results,\n",
    "        'final_metrics': final_metrics,\n",
    "        'model_names': model_names,\n",
    "        'top4_model_ids': top4_model_ids,\n",
    "        'model_f1_scores': model_f1_scores,\n",
    "        'individual_results': individual_results,\n",
    "        'best_shap_model': best_model_name\n",
    "    }\n",
    "\n",
    "    print(f\"{'='*50}{base_path} completed{'='*50}\")\n",
    "\n",
    "# ── 최종 요약 출력 ──────────────────────────────────────────\n",
    "print(\"\\n전체 결과 요약:\")\n",
    "for ratio, res in all_blended_results.items():\n",
    "    print(f\"\\n[{ratio}]\")\n",
    "    print(f\"  블렌딩 모델: {', '.join(res['top4_model_ids'])}\")\n",
    "    if 'F1' in res['blend_results'].columns:\n",
    "        print(f\"  블렌드 평균 F1: {res['blend_results']['F1'].mean():.4f}\")\n",
    "    print(f\"  SHAP Best 모델: {res['best_shap_model']}\")\n",
    "    print(\"  개별 모델 F1:\")\n",
    "    for mid in res['top4_model_ids']:\n",
    "        print(f\"    {mid}: {res['model_f1_scores'][mid]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
