{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e411faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycaret.classification import *\n",
    "import os\n",
    "import joblib\n",
    "import shutil\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='lightgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7ce6d",
   "metadata": {},
   "source": [
    "# 세팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b95986",
   "metadata": {},
   "source": [
    "## 사용할 descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d04a3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_descriptor = pd.read_csv('../data/descriptor_selection.csv')\n",
    "\n",
    "file_md_list = {}\n",
    "for column in selected_descriptor.columns:\n",
    "    filename = column\n",
    "    selected_columns = selected_descriptor[column].iloc[0:].dropna().tolist()\n",
    "    if filename and selected_columns:\n",
    "        file_md_list[filename] = selected_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a81072",
   "metadata": {},
   "source": [
    "## 저장 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75bd54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_move_file(src, dst):\n",
    "    if os.path.exists(src):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "            shutil.move(src, dst)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"파일 이동 실패: {src} -> {dst}, 에러: {e}\")\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def safe_save_csv(df, path):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"CSV 저장 실패: {path}, 에러: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1521974",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_filenames = {\n",
    "    'auc': 'AUC.png',\n",
    "    'confusion_matrix': 'Confusion Matrix.png',\n",
    "    'learning': 'Learning Curve.png',\n",
    "    'feature': 'Feature Importance.png',\n",
    "    'error': 'Prediction Error.png',\n",
    "    'calibration': 'Calibration Curve.png'\n",
    "}\n",
    "\n",
    "target_models = ['lr', 'et', 'gbc', 'lightgbm', 'svm', 'rf', 'ada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba61c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"..\", \"data\", \"preprocessed\")\n",
    "result_dir = os.path.join(\"..\", \"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60bf1e1",
   "metadata": {},
   "source": [
    "# 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1475b",
   "metadata": {},
   "source": [
    "## 개별 모델 학습 및 블랜딩할 모델 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c3b061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================FTO_Final/5x_w3D start==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"c:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\subprocess.py\", line 505, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\subprocess.py\", line 951, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\subprocess.py\", line 1436, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning lr...\n",
      "모델 저장 완료\n",
      "평가 결과 저장 완료\n",
      "auc // AUC.png\n",
      "AUC.png 저장 완료\n",
      "confusion_matrix // Confusion Matrix.png\n",
      "Confusion Matrix.png 저장 완료\n",
      "learning // Learning Curve.png\n",
      "Learning Curve.png 저장 완료\n",
      "feature // Feature Importance.png\n",
      "Feature Importance.png 저장 완료\n",
      "error // Prediction Error.png\n",
      "Prediction Error.png 저장 완료\n",
      "calibration // Calibration Curve.png\n",
      "Calibration Curve.png 저장 완료\n",
      "Tuning et...\n",
      "모델 저장 완료\n",
      "평가 결과 저장 완료\n",
      "auc // AUC.png\n",
      "AUC.png 저장 완료\n",
      "confusion_matrix // Confusion Matrix.png\n",
      "Confusion Matrix.png 저장 완료\n",
      "learning // Learning Curve.png\n",
      "Learning Curve.png 저장 완료\n",
      "feature // Feature Importance.png\n",
      "Feature Importance.png 저장 완료\n",
      "error // Prediction Error.png\n",
      "Prediction Error.png 저장 완료\n",
      "calibration // Calibration Curve.png\n",
      "Calibration Curve.png 저장 완료\n",
      "Tuning gbc...\n",
      "모델 저장 완료\n",
      "평가 결과 저장 완료\n",
      "auc // AUC.png\n",
      "AUC.png 저장 완료\n",
      "confusion_matrix // Confusion Matrix.png\n",
      "Confusion Matrix.png 저장 완료\n",
      "learning // Learning Curve.png\n",
      "Learning Curve.png 저장 완료\n",
      "feature // Feature Importance.png\n",
      "Feature Importance.png 저장 완료\n",
      "error // Prediction Error.png\n",
      "Prediction Error.png 저장 완료\n",
      "calibration // Calibration Curve.png\n",
      "Calibration Curve.png 저장 완료\n",
      "Tuning lightgbm...\n",
      "모델 저장 완료\n",
      "평가 결과 저장 완료\n",
      "auc // AUC.png\n",
      "AUC.png 저장 완료\n",
      "confusion_matrix // Confusion Matrix.png\n",
      "Confusion Matrix.png 저장 완료\n",
      "learning // Learning Curve.png\n",
      "Learning Curve.png 저장 완료\n",
      "feature // Feature Importance.png\n",
      "Feature Importance.png 저장 완료\n",
      "error // Prediction Error.png\n",
      "Prediction Error.png 저장 완료\n",
      "calibration // Calibration Curve.png\n",
      "Calibration Curve.png 저장 완료\n",
      "Tuning svm...\n",
      "모델 저장 완료\n",
      "평가 결과 저장 완료\n",
      "auc // AUC.png\n",
      "[auc] Plot 생성 실패: AUC plot not available for estimators with no predict_proba attribute.\n",
      "에러 정보가 5x_SGDClassifier_auc_error.txt에 저장되었습니다.\n",
      "confusion_matrix // Confusion Matrix.png\n",
      "Confusion Matrix.png 저장 완료\n",
      "learning // Learning Curve.png\n",
      "Learning Curve.png 저장 완료\n",
      "feature // Feature Importance.png\n",
      "Feature Importance.png 저장 완료\n",
      "error // Prediction Error.png\n",
      "Prediction Error.png 저장 완료\n",
      "calibration // Calibration Curve.png\n",
      "[calibration] Plot 생성 실패: Calibration plot not available for estimators with no predict_proba attribute.\n",
      "에러 정보가 5x_SGDClassifier_calibration_error.txt에 저장되었습니다.\n",
      "Tuning rf...\n",
      "모델 저장 완료\n",
      "평가 결과 저장 완료\n",
      "auc // AUC.png\n",
      "AUC.png 저장 완료\n",
      "confusion_matrix // Confusion Matrix.png\n",
      "Confusion Matrix.png 저장 완료\n",
      "learning // Learning Curve.png\n",
      "Learning Curve.png 저장 완료\n",
      "feature // Feature Importance.png\n",
      "Feature Importance.png 저장 완료\n",
      "error // Prediction Error.png\n",
      "Prediction Error.png 저장 완료\n",
      "calibration // Calibration Curve.png\n",
      "Calibration Curve.png 저장 완료\n",
      "Tuning ada...\n",
      "모델 저장 완료\n",
      "평가 결과 저장 완료\n",
      "auc // AUC.png\n",
      "AUC.png 저장 완료\n",
      "confusion_matrix // Confusion Matrix.png\n",
      "Confusion Matrix.png 저장 완료\n",
      "learning // Learning Curve.png\n",
      "Learning Curve.png 저장 완료\n",
      "feature // Feature Importance.png\n",
      "Feature Importance.png 저장 완료\n",
      "error // Prediction Error.png\n",
      "Prediction Error.png 저장 완료\n",
      "calibration // Calibration Curve.png\n",
      "Calibration Curve.png 저장 완료\n",
      "==================================================FTO_Final/5x_w3D completed==================================================\n",
      "==================================================FTO_Final/10x_w3D start==================================================\n",
      "Tuning lr...\n",
      "모델 저장 완료\n",
      "평가 결과 저장 완료\n",
      "auc // AUC.png\n",
      "AUC.png 저장 완료\n",
      "confusion_matrix // Confusion Matrix.png\n",
      "Confusion Matrix.png 저장 완료\n",
      "learning // Learning Curve.png\n",
      "Learning Curve.png 저장 완료\n",
      "feature // Feature Importance.png\n",
      "Feature Importance.png 저장 완료\n",
      "error // Prediction Error.png\n",
      "Prediction Error.png 저장 완료\n",
      "calibration // Calibration Curve.png\n",
      "Calibration Curve.png 저장 완료\n",
      "Tuning et...\n",
      "모델 저장 완료\n",
      "평가 결과 저장 완료\n",
      "auc // AUC.png\n",
      "AUC.png 저장 완료\n",
      "confusion_matrix // Confusion Matrix.png\n",
      "Confusion Matrix.png 저장 완료\n",
      "learning // Learning Curve.png\n",
      "Learning Curve.png 저장 완료\n",
      "feature // Feature Importance.png\n",
      "Feature Importance.png 저장 완료\n",
      "error // Prediction Error.png\n",
      "Prediction Error.png 저장 완료\n",
      "calibration // Calibration Curve.png\n",
      "Calibration Curve.png 저장 완료\n",
      "Tuning gbc...\n",
      "모델 저장 완료\n",
      "평가 결과 저장 완료\n",
      "auc // AUC.png\n",
      "AUC.png 저장 완료\n",
      "confusion_matrix // Confusion Matrix.png\n",
      "Confusion Matrix.png 저장 완료\n",
      "learning // Learning Curve.png\n",
      "Learning Curve.png 저장 완료\n",
      "feature // Feature Importance.png\n",
      "Feature Importance.png 저장 완료\n",
      "error // Prediction Error.png\n",
      "Prediction Error.png 저장 완료\n",
      "calibration // Calibration Curve.png\n",
      "Calibration Curve.png 저장 완료\n",
      "Tuning lightgbm...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(model_id, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m tuned_model \u001b[38;5;241m=\u001b[39m \u001b[43mtune_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mF1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchoose_better\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m results \u001b[38;5;241m=\u001b[39m pull()\n\u001b[0;32m     55\u001b[0m model_name \u001b[38;5;241m=\u001b[39m tuned_model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\site-packages\\pycaret\\utils\\generic.py:970\u001b[0m, in \u001b[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m globals_d[name] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    969\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\site-packages\\pycaret\\classification\\functional.py:1215\u001b[0m, in \u001b[0;36mtune_model\u001b[1;34m(estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;129m@check_if_global_is_not_none\u001b[39m(\u001b[38;5;28mglobals\u001b[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtune_model\u001b[39m(\n\u001b[0;32m   1026\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1045\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;124;03m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \n\u001b[0;32m   1213\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _CURRENT_EXPERIMENT\u001b[38;5;241m.\u001b[39mtune_model(\n\u001b[0;32m   1216\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1217\u001b[0m         fold\u001b[38;5;241m=\u001b[39mfold,\n\u001b[0;32m   1218\u001b[0m         \u001b[38;5;28mround\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m,\n\u001b[0;32m   1219\u001b[0m         n_iter\u001b[38;5;241m=\u001b[39mn_iter,\n\u001b[0;32m   1220\u001b[0m         custom_grid\u001b[38;5;241m=\u001b[39mcustom_grid,\n\u001b[0;32m   1221\u001b[0m         optimize\u001b[38;5;241m=\u001b[39moptimize,\n\u001b[0;32m   1222\u001b[0m         custom_scorer\u001b[38;5;241m=\u001b[39mcustom_scorer,\n\u001b[0;32m   1223\u001b[0m         search_library\u001b[38;5;241m=\u001b[39msearch_library,\n\u001b[0;32m   1224\u001b[0m         search_algorithm\u001b[38;5;241m=\u001b[39msearch_algorithm,\n\u001b[0;32m   1225\u001b[0m         early_stopping\u001b[38;5;241m=\u001b[39mearly_stopping,\n\u001b[0;32m   1226\u001b[0m         early_stopping_max_iters\u001b[38;5;241m=\u001b[39mearly_stopping_max_iters,\n\u001b[0;32m   1227\u001b[0m         choose_better\u001b[38;5;241m=\u001b[39mchoose_better,\n\u001b[0;32m   1228\u001b[0m         fit_kwargs\u001b[38;5;241m=\u001b[39mfit_kwargs,\n\u001b[0;32m   1229\u001b[0m         groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m   1230\u001b[0m         return_tuner\u001b[38;5;241m=\u001b[39mreturn_tuner,\n\u001b[0;32m   1231\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   1232\u001b[0m         tuner_verbose\u001b[38;5;241m=\u001b[39mtuner_verbose,\n\u001b[0;32m   1233\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m   1234\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1235\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\site-packages\\pycaret\\classification\\oop.py:1565\u001b[0m, in \u001b[0;36mClassificationExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtune_model\u001b[39m(\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1376\u001b[0m     estimator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1394\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1395\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1396\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;124;03m    This function tunes the hyperparameters of a given estimator. The output of\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m \u001b[38;5;124;03m    this function is a score grid with CV scores by fold of the best selected\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1562\u001b[0m \n\u001b[0;32m   1563\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtune_model(\n\u001b[0;32m   1566\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1567\u001b[0m         fold\u001b[38;5;241m=\u001b[39mfold,\n\u001b[0;32m   1568\u001b[0m         \u001b[38;5;28mround\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m,\n\u001b[0;32m   1569\u001b[0m         n_iter\u001b[38;5;241m=\u001b[39mn_iter,\n\u001b[0;32m   1570\u001b[0m         custom_grid\u001b[38;5;241m=\u001b[39mcustom_grid,\n\u001b[0;32m   1571\u001b[0m         optimize\u001b[38;5;241m=\u001b[39moptimize,\n\u001b[0;32m   1572\u001b[0m         custom_scorer\u001b[38;5;241m=\u001b[39mcustom_scorer,\n\u001b[0;32m   1573\u001b[0m         search_library\u001b[38;5;241m=\u001b[39msearch_library,\n\u001b[0;32m   1574\u001b[0m         search_algorithm\u001b[38;5;241m=\u001b[39msearch_algorithm,\n\u001b[0;32m   1575\u001b[0m         early_stopping\u001b[38;5;241m=\u001b[39mearly_stopping,\n\u001b[0;32m   1576\u001b[0m         early_stopping_max_iters\u001b[38;5;241m=\u001b[39mearly_stopping_max_iters,\n\u001b[0;32m   1577\u001b[0m         choose_better\u001b[38;5;241m=\u001b[39mchoose_better,\n\u001b[0;32m   1578\u001b[0m         fit_kwargs\u001b[38;5;241m=\u001b[39mfit_kwargs,\n\u001b[0;32m   1579\u001b[0m         groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m   1580\u001b[0m         return_tuner\u001b[38;5;241m=\u001b[39mreturn_tuner,\n\u001b[0;32m   1581\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   1582\u001b[0m         tuner_verbose\u001b[38;5;241m=\u001b[39mtuner_verbose,\n\u001b[0;32m   1583\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m   1584\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1585\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:2714\u001b[0m, in \u001b[0;36m_SupervisedExperiment.tune_model\u001b[1;34m(self, estimator, fold, round, n_iter, custom_grid, optimize, custom_scorer, search_library, search_algorithm, early_stopping, early_stopping_max_iters, choose_better, fit_kwargs, groups, return_tuner, verbose, tuner_verbose, return_train_score, **kwargs)\u001b[0m\n\u001b[0;32m   2706\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch(\n\u001b[0;32m   2707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn.model_selection._search.sample_without_replacement\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2708\u001b[0m         pycaret\u001b[38;5;241m.\u001b[39minternal\u001b[38;5;241m.\u001b[39mpatches\u001b[38;5;241m.\u001b[39msklearn\u001b[38;5;241m.\u001b[39m_mp_sample_without_replacement,\n\u001b[0;32m   2709\u001b[0m     ):\n\u001b[0;32m   2710\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m patch(\n\u001b[0;32m   2711\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn.model_selection._search.ParameterGrid.__getitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2712\u001b[0m             pycaret\u001b[38;5;241m.\u001b[39minternal\u001b[38;5;241m.\u001b[39mpatches\u001b[38;5;241m.\u001b[39msklearn\u001b[38;5;241m.\u001b[39m_mp_ParameterGrid_getitem,\n\u001b[0;32m   2713\u001b[0m         ):\n\u001b[1;32m-> 2714\u001b[0m             model_grid\u001b[38;5;241m.\u001b[39mfit(data_X, data_y, groups\u001b[38;5;241m=\u001b[39mgroups, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m   2715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2716\u001b[0m     model_grid\u001b[38;5;241m.\u001b[39mfit(data_X, data_y, groups\u001b[38;5;241m=\u001b[39mgroups, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1914\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1914\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1916\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\cuda\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for ratio in ['5x', '10x']:\n",
    "    file_name = f'descriptors_filtered_FTO_training_{ratio}_ignore3D_False.csv'\n",
    "    base_path = f'FTO_Final/{ratio}_w3D'\n",
    "    \n",
    "    # 데이터 파일 경로\n",
    "    data_path = os.path.join(data_dir, f\"filtered_FTO_training_{ratio}_ignore3D_False.csv\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"{'='*50}{base_path} start{'='*50}\")\n",
    "\n",
    "    # 저장 폴더 생성\n",
    "    full_result_path = os.path.join(result_dir, base_path)\n",
    "    models_dir = os.path.join(full_result_path, \"models\")\n",
    "    plots_dir = os.path.join(full_result_path, \"plots\")\n",
    "    \n",
    "    for dir_path in [full_result_path, models_dir, plots_dir]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    md_cols = file_md_list[file_name]\n",
    "    fp_cols = [f'X{i+1}' for i in range(1024)]\n",
    "    filtered_df = df[['potency'] + fp_cols + md_cols]\n",
    "\n",
    "    X = filtered_df.drop('potency', axis=1)\n",
    "    Y = filtered_df['potency']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42, stratify=Y)\n",
    "    df_train = pd.concat([x_train, y_train], axis=1)\n",
    "\n",
    "    setup(\n",
    "        data=df_train, \n",
    "        target='potency',\n",
    "        session_id=42,\n",
    "        train_size=0.9,\n",
    "        fold=10,\n",
    "        normalize=True,\n",
    "        fix_imbalance=True,\n",
    "        remove_outliers=True,\n",
    "        n_jobs=-1, \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    summary_data = []\n",
    "    for model_id in target_models:\n",
    "        model = create_model(model_id, verbose=False)\n",
    "        print(f\"Tuning {model_id}...\")\n",
    "        \n",
    "        tuned_model = tune_model(\n",
    "            model, \n",
    "            optimize='F1',\n",
    "            n_iter=50,\n",
    "            fold=5,\n",
    "            choose_better=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        results = pull()\n",
    "        model_name = tuned_model.__class__.__name__\n",
    "        \n",
    "        # 모델 저장\n",
    "        model_path = os.path.join(models_dir, f\"{ratio}_{model_name}_model.pkl\")\n",
    "        joblib.dump(tuned_model, model_path)\n",
    "        print(f\"모델 저장 완료\")\n",
    "        \n",
    "        # 평가 결과 저장\n",
    "        eval_path = os.path.join(full_result_path, f\"{ratio}_{model_name}_evaluation.csv\")\n",
    "        safe_save_csv(results, eval_path)\n",
    "        print(f\"평가 결과 저장 완료\")\n",
    "        \n",
    "        # 플롯 생성 및 저장\n",
    "        for plot_type, default_name in default_filenames.items():\n",
    "            try:\n",
    "                print(f\"{plot_type} // {default_name}\")\n",
    "                plot_model(tuned_model, plot=plot_type, save=True, verbose=False)\n",
    "                \n",
    "                if os.path.exists(default_name):\n",
    "                    final_filename = f\"{ratio}_{model_name}_{plot_type}.png\"\n",
    "                    final_save_path = os.path.join(plots_dir, final_filename)\n",
    "                    shutil.move(default_name, final_save_path)\n",
    "                    print(f\"{default_name} 저장 완료\")\n",
    "                else:\n",
    "                    print(f\"{default_name} 파일이 생성되지 않았습니다.\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"[{plot_type}] Plot 생성 실패: {e}\")\n",
    "                \n",
    "                error_filename = f\"{ratio}_{model_name}_{plot_type}_error.txt\"\n",
    "                error_path = os.path.join(plots_dir, error_filename)\n",
    "                with open(error_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"Plot Type: {plot_type}\\n\")\n",
    "                    f.write(f\"Model: {model_name}\\n\")\n",
    "                    f.write(f\"Error: {str(e)}\\n\")\n",
    "                print(f\"에러 정보가 {error_filename}에 저장되었습니다.\")\n",
    "        \n",
    "        numeric_cols = results.select_dtypes(include=[np.number]).columns\n",
    "        avg_row = results[numeric_cols].mean().to_dict()\n",
    "        std_row = results[numeric_cols].std().to_dict()\n",
    "        \n",
    "        avg_row.update({'Model': model_name, 'Type': 'Mean'})\n",
    "        std_row.update({'Model': model_name, 'Type': 'Std'})\n",
    "        \n",
    "        summary_data.extend([avg_row, std_row])\n",
    "\n",
    "    # 전체 요약 저장\n",
    "    if summary_data:\n",
    "        combined_summary = pd.DataFrame(summary_data)\n",
    "        summary_path = os.path.join(full_result_path, f\"{ratio}_summary_evaluation.csv\")\n",
    "        safe_save_csv(combined_summary, summary_path)\n",
    "\n",
    "    print(f\"{'='*50}{base_path} completed{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59abae",
   "metadata": {},
   "source": [
    "## 블랜딩 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9f20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================5x Blending Start==============================\n",
      "Creating fresh et model...\n",
      "et tuning completed\n",
      "Creating fresh gbc model...\n",
      "gbc tuning completed\n",
      "Creating fresh lightgbm model...\n",
      "lightgbm tuning completed\n",
      "Creating fresh lr model...\n",
      "lr tuning completed\n",
      "Attempting to create blend model...\n",
      "블렌드 모델 저장 완료: ..\\result\\FTO_Final/5x_w3D\\blend_models\\5x_blended_model2.pkl\n",
      "최종 예측 완료\n",
      "Generating auc plot...\n",
      "auc 플롯 저장 완료\n",
      "Generating confusion_matrix plot...\n",
      "confusion_matrix 플롯 저장 완료\n",
      "Generating learning plot...\n",
      "learning 플롯 저장 완료\n",
      "Generating feature plot...\n",
      "[feature] Plot 생성 실패: Feature Importance and RFE plots not available for estimators that doesnt support coef_ or feature_importances_ attribute.\n",
      "블렌딩 성공: ExtraTreesClassifier, GradientBoostingClassifier, LGBMClassifier, LogisticRegression\n",
      "==============================5x Blending Complete==============================\n",
      "==============================10x Blending Start==============================\n",
      "Creating fresh et model...\n",
      "et tuning completed\n",
      "Creating fresh gbc model...\n",
      "gbc tuning completed\n",
      "Creating fresh lightgbm model...\n",
      "lightgbm tuning completed\n",
      "Creating fresh lr model...\n",
      "lr tuning completed\n",
      "Attempting to create blend model...\n",
      "블렌드 모델 저장 완료: ..\\result\\FTO_Final/10x_w3D\\blend_models\\10x_blended_model2.pkl\n",
      "최종 예측 완료\n",
      "Generating auc plot...\n",
      "auc 플롯 저장 완료\n",
      "Generating confusion_matrix plot...\n",
      "confusion_matrix 플롯 저장 완료\n",
      "Generating learning plot...\n",
      "learning 플롯 저장 완료\n",
      "Generating feature plot...\n",
      "[feature] Plot 생성 실패: Feature Importance and RFE plots not available for estimators that doesnt support coef_ or feature_importances_ attribute.\n",
      "블렌딩 성공: ExtraTreesClassifier, GradientBoostingClassifier, LGBMClassifier, LogisticRegression\n",
      "==============================10x Blending Complete==============================\n",
      "\n",
      "블렌딩 결과 요약:\n",
      "\n",
      "[5x] Blend Model:\n",
      "  - 사용된 모델: ExtraTreesClassifier, GradientBoostingClassifier, LGBMClassifier, LogisticRegression\n",
      "  - 블렌드 모델 평균 F1: 0.6836\n",
      "  - 개별 모델 F1 점수:\n",
      "    et: 0.7303\n",
      "    gbc: 0.7353\n",
      "    lightgbm: 0.7186\n",
      "    lr: 0.7320\n",
      "\n",
      "[10x] Blend Model:\n",
      "  - 사용된 모델: ExtraTreesClassifier, GradientBoostingClassifier, LGBMClassifier, LogisticRegression\n",
      "  - 블렌드 모델 평균 F1: 0.6613\n",
      "  - 개별 모델 F1 점수:\n",
      "    et: 0.7102\n",
      "    gbc: 0.7195\n",
      "    lightgbm: 0.7125\n",
      "    lr: 0.7299\n",
      "\n",
      "모든 블렌딩 작업이 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "blend_models_list = ['et', 'gbc', 'lightgbm', 'lr']\n",
    "\n",
    "all_blended_results = {}\n",
    "\n",
    "for ratio in ['5x', '10x']:\n",
    "    file_name = f'descriptors_filtered_FTO_training_{ratio}_ignore3D_False.csv'\n",
    "    base_path = f'FTO_Final/{ratio}_w3D'\n",
    "    \n",
    "    data_path = os.path.join(data_dir, f\"filtered_FTO_training_{ratio}_ignore3D_False.csv\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"{'='*30}{ratio} Blending Start{'='*30}\")\n",
    "\n",
    "    full_result_path = os.path.join(result_dir, base_path)\n",
    "    blend_models_dir = os.path.join(full_result_path, \"blend_models\")\n",
    "    blend_plots_dir = os.path.join(full_result_path, \"blend_plots\")\n",
    "    \n",
    "    for dir_path in [blend_models_dir, blend_plots_dir]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    md_cols = file_md_list[file_name]\n",
    "    fp_cols = [f'X{i+1}' for i in range(1024)]\n",
    "    filtered_df = df[['potency'] + fp_cols + md_cols]\n",
    "\n",
    "    X = filtered_df.drop('potency', axis=1)\n",
    "    Y = filtered_df['potency']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42, stratify=Y)\n",
    "    df_train = pd.concat([x_train, y_train], axis=1)\n",
    "\n",
    "    exp = setup(\n",
    "        data=df_train, \n",
    "        target='potency',\n",
    "        session_id=42,\n",
    "        train_size=0.9,\n",
    "        fold=10,\n",
    "        normalize=True,\n",
    "        fix_imbalance=True,\n",
    "        remove_outliers=True,\n",
    "        n_jobs=1,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    fresh_models = []\n",
    "    model_names = []\n",
    "    individual_results = {}\n",
    "    \n",
    "    for model_id in blend_models_list:\n",
    "        print(f\"Creating fresh {model_id} model...\")\n",
    "        if model_id == 'lightgbm':\n",
    "            lgb_params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 31,\n",
    "                'learning_rate': 0.1,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'min_child_samples': 20,\n",
    "                'random_state': 42,\n",
    "                'n_estimators': 100,\n",
    "                'verbosity': -1\n",
    "            }\n",
    "            fresh_model = create_model(model_id, verbose=False, **lgb_params)\n",
    "        else:\n",
    "            fresh_model = create_model(model_id, verbose=False)\n",
    "            \n",
    "        tuned_model = tune_model(\n",
    "            fresh_model, \n",
    "            optimize='F1',\n",
    "            n_iter=50,\n",
    "            fold=5,\n",
    "            choose_better=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        model_results = pull()\n",
    "        individual_results[model_id] = model_results\n",
    "        \n",
    "        fresh_models.append(tuned_model)\n",
    "        model_names.append(tuned_model.__class__.__name__)\n",
    "        \n",
    "        print(f\"{model_id} tuning completed\")        \n",
    "    \n",
    "    print(\"Attempting to create blend model...\")\n",
    "    blend_success = False\n",
    "    \n",
    "    blend_attempts = [\n",
    "            ]\n",
    "    \n",
    "    blended_model = blend_models(\n",
    "        estimator_list=fresh_models,\n",
    "        verbose=False,\n",
    "        **{'fold': 3, 'method': 'soft'},\n",
    "    )\n",
    "    blend_results = pull()\n",
    "    blend_success = True\n",
    "\n",
    "    blend_model_path = os.path.join(blend_models_dir, f\"{ratio}_blended_model2.pkl\")\n",
    "    joblib.dump(blended_model, blend_model_path)\n",
    "    print(f\"블렌드 모델 저장 완료: {blend_model_path}\")\n",
    "    \n",
    "    blend_eval_path = os.path.join(full_result_path, f\"{ratio}_blend_evaluation.csv\")\n",
    "    safe_save_csv(blend_results, blend_eval_path)\n",
    "    \n",
    "    # 테스트 데이터 예측\n",
    "    test_data = pd.concat([x_test, y_test], axis=1)\n",
    "    try:\n",
    "        final_predictions = predict_model(blended_model, data=test_data, verbose=False)\n",
    "        final_metrics = pull()\n",
    "        final_metrics_path = os.path.join(full_result_path, f\"{ratio}_blend_final_metrics.csv\")\n",
    "        safe_save_csv(final_metrics, final_metrics_path)\n",
    "        print(\"최종 예측 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"Final prediction failed: {e}\")\n",
    "        final_metrics = None\n",
    "    \n",
    "    # 플롯 생성\n",
    "    plot_types = ['auc', 'confusion_matrix', 'learning', 'feature']\n",
    "    for plot_type in plot_types:\n",
    "        try:\n",
    "            print(f\"Generating {plot_type} plot...\")\n",
    "            plot_model(blended_model, plot=plot_type, save=True, verbose=False)\n",
    "            \n",
    "            default_name = default_filenames.get(plot_type, f'{plot_type}.png')\n",
    "            if os.path.exists(default_name):\n",
    "                final_filename = f\"{ratio}_blend_{plot_type}.png\"\n",
    "                final_save_path = os.path.join(blend_plots_dir, final_filename)\n",
    "                safe_move_file(default_name, final_save_path)\n",
    "                print(f\"{plot_type} 플롯 저장 완료\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[{plot_type}] Plot 생성 실패: {e}\")\n",
    "            error_filename = f\"{ratio}_blend_{plot_type}_error.txt\"\n",
    "            error_path = os.path.join(blend_plots_dir, error_filename)\n",
    "            with open(error_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"Plot Type: {plot_type}\\nModel: Blended\\nError: {str(e)}\\n\")\n",
    "    \n",
    "    # 개별 모델 결과 저장\n",
    "    for model_id, results in individual_results.items():\n",
    "        individual_path = os.path.join(full_result_path, f\"{ratio}_{model_id}_individual_results.csv\")\n",
    "        safe_save_csv(results, individual_path)\n",
    "    \n",
    "    all_blended_results[ratio] = {\n",
    "        'blend_results': blend_results,\n",
    "        'final_metrics': final_metrics,\n",
    "        'model_names': model_names,\n",
    "        'individual_results': individual_results\n",
    "    }\n",
    "    \n",
    "    print(f\"블렌딩 성공: {', '.join(model_names)}\")    \n",
    "    print(f\"{'='*30}{ratio} Blending Complete{'='*30}\")\n",
    "\n",
    "print(\"\\n블렌딩 결과 요약:\")\n",
    "for ratio, results in all_blended_results.items():\n",
    "    print(f\"\\n[{ratio}] Blend Model:\")\n",
    "    print(f\"  - 사용된 모델: {', '.join(results['model_names'])}\")\n",
    "    if 'blend_results' in results and results['blend_results'] is not None:\n",
    "        if 'F1' in results['blend_results'].columns:\n",
    "            blend_f1 = results['blend_results']['F1'].mean()\n",
    "            print(f\"  - 블렌드 모델 평균 F1: {blend_f1:.4f}\")\n",
    "    print(\"  - 개별 모델 F1 점수:\")\n",
    "    for model_id, result_df in results['individual_results'].items():\n",
    "        if 'F1' in result_df.columns:\n",
    "            individual_f1 = result_df['F1'].mean()\n",
    "            print(f\"    {model_id}: {individual_f1:.4f}\")\n",
    "\n",
    "print(\"\\n모든 블렌딩 작업이 완료되었습니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd753b09",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb295e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================== 5x ==================================================\n",
      "Best 모델: LogisticRegression (AUC: 0.8310)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb7b53ac723481a9aa0f7eab912751a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimating transforms:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearExplainer 사용\n",
      "완료: 5x - LogisticRegression SHAP 저장 -> ../result/FTO_Final/5x_w3D/SHAP\n",
      "\n",
      "================================================== 10x ==================================================\n",
      "Best 모델: GradientBoostingClassifier (AUC: 0.8374)\n",
      "TreeExplainer 사용\n",
      "완료: 10x - GradientBoostingClassifier SHAP 저장 -> ../result/FTO_Final/10x_w3D/SHAP\n"
     ]
    }
   ],
   "source": [
    "for ratio in ['5x', '10x']:\n",
    "    print(f\"\\n{'='*50} {ratio} {'='*50}\")\n",
    "\n",
    "    base_path = f'FTO_Final/{ratio}_w3D'\n",
    "    full_result_path = os.path.join(result_dir, base_path)\n",
    "    shap_dir = os.path.join(full_result_path, \"SHAP\")\n",
    "    os.makedirs(shap_dir, exist_ok=True)\n",
    "\n",
    "    # 데이터 로드\n",
    "    file_name = f'descriptors_filtered_FTO_training_{ratio}_ignore3D_False.csv'\n",
    "    data_path = os.path.join(data_dir, f\"filtered_FTO_training_{ratio}_ignore3D_False.csv\")\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    md_cols = file_md_list[file_name]\n",
    "    fp_cols = [f'X{i+1}' for i in range(1024)]\n",
    "    filtered_df = df[['potency'] + fp_cols + md_cols]\n",
    "\n",
    "    X = filtered_df.drop('potency', axis=1)\n",
    "    Y = filtered_df['potency']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        X, Y, test_size=0.1, random_state=42, stratify=Y\n",
    "    )\n",
    "    df_train = pd.concat([x_train, y_train], axis=1)\n",
    "\n",
    "    # PyCaret setup (전처리 파이프라인 복원용)\n",
    "    exp = setup(\n",
    "        data=df_train,\n",
    "        target='potency',\n",
    "        session_id=42,\n",
    "        train_size=0.9,\n",
    "        fold=10,\n",
    "        normalize=True,\n",
    "        fix_imbalance=True,\n",
    "        remove_outliers=True,\n",
    "        n_jobs=1,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # 전처리 파이프라인으로 변환\n",
    "    pipeline = get_config('pipeline')\n",
    "    X_test_transformed = pipeline.transform(x_test)\n",
    "    X_train_transformed = pipeline.transform(x_train)\n",
    "\n",
    "    if hasattr(X_test_transformed, 'columns'):\n",
    "        feature_names = X_test_transformed.columns.tolist()\n",
    "        X_test_df = X_test_transformed\n",
    "        X_train_df = X_train_transformed\n",
    "    else:\n",
    "        feature_names = [f'f{i}' for i in range(X_test_transformed.shape[1])]\n",
    "        X_test_df = pd.DataFrame(X_test_transformed, columns=feature_names)\n",
    "        X_train_df = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
    "\n",
    "    # Best 모델 선택 (AUC 기준)\n",
    "    summary_path = os.path.join(full_result_path, f\"{ratio}_summary_evaluation.csv\")\n",
    "    summary_df = pd.read_csv(summary_path)\n",
    "    mean_df = summary_df[summary_df['Type'] == 'Mean'].copy()\n",
    "    best_row = mean_df.loc[mean_df['AUC'].idxmax()]\n",
    "    best_model_name = best_row['Model']\n",
    "    print(f\"Best 모델: {best_model_name} (AUC: {best_row['AUC']:.4f})\")\n",
    "\n",
    "    # 모델 로드 (joblib)\n",
    "    model_path = os.path.join(full_result_path, \"models\", f\"{ratio}_{best_model_name}_model.pkl\")\n",
    "    best_model = joblib.load(model_path)\n",
    "\n",
    "    # SHAP 계산\n",
    "    if 'LogisticRegression' in best_model_name:\n",
    "        explainer = shap.LinearExplainer(\n",
    "            best_model,\n",
    "            X_train_df,\n",
    "            feature_perturbation=\"correlation_dependent\"\n",
    "        )\n",
    "        shap_values_class1 = explainer.shap_values(X_test_df)\n",
    "        print(\"LinearExplainer 사용\")\n",
    "    else:\n",
    "        try:\n",
    "            explainer = shap.TreeExplainer(best_model)\n",
    "            shap_values = explainer.shap_values(X_test_df)\n",
    "            shap_values_class1 = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
    "            print(\"TreeExplainer 사용\")\n",
    "        except Exception as e:\n",
    "            print(f\"TreeExplainer 실패: {e} -> KernelExplainer 사용\")\n",
    "            X_background = shap.sample(X_train_df, 50)\n",
    "            explainer = shap.KernelExplainer(best_model.predict_proba, X_background)\n",
    "            shap_values = explainer.shap_values(X_test_df, nsamples=100)\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values_class1 = shap_values[1]\n",
    "            elif isinstance(shap_values, np.ndarray) and shap_values.ndim == 3:\n",
    "                shap_values_class1 = shap_values[:, :, 1]\n",
    "            else:\n",
    "                shap_values_class1 = shap_values\n",
    "            print(\"KernelExplainer 사용\")\n",
    "            \n",
    "    # Summary Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values_class1, X_test_df,\n",
    "                      feature_names=feature_names, show=False, max_display=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(shap_dir, f'shap_{best_model_name}_summary.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Bar Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_class1, X_test_df,\n",
    "                      feature_names=feature_names, plot_type='bar', show=False, max_display=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(shap_dir, f'shap_{best_model_name}_bar.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Feature Importance CSV\n",
    "    mean_abs_shap = np.abs(shap_values_class1).mean(axis=0)\n",
    "    pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'mean_abs_shap': mean_abs_shap\n",
    "    }).sort_values('mean_abs_shap', ascending=False).to_csv(\n",
    "        os.path.join(shap_dir, f'shap_{best_model_name}_importance.csv'), index=False\n",
    "    )\n",
    "\n",
    "    print(f\"완료: {ratio} - {best_model_name} SHAP 저장 -> {shap_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
