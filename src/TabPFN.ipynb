{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eddee9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib.backends.registry'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\n\u001b[32m     24\u001b[39m huggingface_hub.login()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\qsar\\Lib\\site-packages\\matplotlib\\__init__.py:161\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrcsetup\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cycler  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\qsar\\Lib\\site-packages\\matplotlib\\rcsetup.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\anaconda3\\envs\\qsar\\Lib\\site-packages\\matplotlib\\backends\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# NOTE: plt.switch_backend() (called at import time) will add a \"backend\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# attribute here for backcompat.\u001b[39;00m\n\u001b[32m      5\u001b[39m _QT_FORCE_QT5_BINDING = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib.backends.registry'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE\n",
    "import shap\n",
    "from tabpfn import TabPFNClassifier\n",
    "from tabpfn.constants import ModelVersion\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93516af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_descriptor = pd.read_csv('../data/descriptor_selection.csv')\n",
    "\n",
    "file_md_list = {}\n",
    "for column in selected_descriptor.columns:\n",
    "    filename = column\n",
    "    selected_columns = selected_descriptor[column].iloc[0:].dropna().tolist()\n",
    "    if filename and selected_columns:\n",
    "        file_md_list[filename] = selected_columns\n",
    "\n",
    "data_dir = os.path.join(\"..\", \"data\", \"preprocessed\")\n",
    "result_dir = os.path.join(\"..\", \"result\", \"FTO_TABPFN\")\n",
    "os.makedirs(result_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c45a5f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 변수 갯수: 1040\n",
      "  - Fingerprint: 1024개\n",
      "  - Molecular Descriptor: 16개\n"
     ]
    }
   ],
   "source": [
    "ratio = '10x'\n",
    "results = {}\n",
    "\n",
    "file_name = f'descriptors_filtered_FTO_training_{ratio}_ignore3D_False.csv'\n",
    "data_path = os.path.join(data_dir, f\"filtered_FTO_training_{ratio}_ignore3D_False.csv\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "md_cols = file_md_list[file_name]\n",
    "fp_cols = [f'X{i+1}' for i in range(1024)]\n",
    "filtered_df = df[['potency'] + fp_cols + md_cols]\n",
    "\n",
    "X = filtered_df.drop('potency', axis=1)\n",
    "Y = filtered_df['potency']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size=0.1, random_state=42, stratify=Y)\n",
    "\n",
    "print(f'현재 변수 갯수: {len(X_train.columns)}')\n",
    "print(f'  - Fingerprint: {len(fp_cols)}개')\n",
    "print(f'  - Molecular Descriptor: {len(md_cols)}개')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686544a",
   "metadata": {},
   "source": [
    "# 기존 Pycaret 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16f65994",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_list = []\n",
    "\n",
    "# PyCaret Blend 결과\n",
    "pycaret_blend_path = f\"../result/FTO_MACCS/{ratio}_w3D/{ratio}_blend_final_metrics.csv\"\n",
    "blend_metrics = pd.read_csv(pycaret_blend_path)\n",
    "comparison_list.append(pd.DataFrame({\n",
    "    'method': ['Blend (PyCaret)'],\n",
    "    'f1': [blend_metrics['F1'].iloc[0]],\n",
    "    'auc': [blend_metrics['AUC'].iloc[0]],\n",
    "    'n_features': [len(X_train.columns)]\n",
    "}))\n",
    "        \n",
    "# PyCaret 개별 모델 결과\n",
    "pycaret_summary_path = f\"../result/FTO_MACCS/{ratio}_w3D/{ratio}_summary_evaluation.csv\"\n",
    "summary_df = pd.read_csv(pycaret_summary_path)\n",
    "mean_df = summary_df[summary_df['Type'] == 'Mean'].copy()\n",
    "\n",
    "for _, row in mean_df.iterrows():\n",
    "    comparison_list.append(pd.DataFrame({\n",
    "        'method': [f\"{row['Model']} (PyCaret)\"],\n",
    "        'f1': [row['F1']],\n",
    "        'auc': [row['AUC']],\n",
    "        'n_features': [len(X_train.columns)]\n",
    "        }))\n",
    "\n",
    "final_comparison_df = pd.concat(comparison_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6619d2",
   "metadata": {},
   "source": [
    "# TabPFN 모델 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3b6c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "n_features_list = [50, 100, 200, 300, 400, 500]\n",
    "feature_methods = ['all', 'random', 'mi', 'pca', 'rfe', 'shap', 'md_only']\n",
    "\n",
    "results = []\n",
    "saved_models = {}\n",
    "\n",
    "model_save_dir = os.path.join(result_dir, 'models')\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(model_save_dir) or not any(f.endswith('.joblib') for f in os.listdir(model_save_dir)):\n",
    "    for method in feature_methods:\n",
    "        print(f\"\\n[{method.upper()} Method]\")\n",
    "\n",
    "        # ── MD only ──────────────────────────────────────────────\n",
    "        if method == 'md_only':\n",
    "            filtered_df_md = df[['potency'] + md_cols]\n",
    "            X_md = filtered_df_md.drop('potency', axis=1)\n",
    "            Y_md = filtered_df_md['potency']\n",
    "\n",
    "            X_train_md, X_test_md, y_train_md, y_test_md = train_test_split(\n",
    "                X_md, Y_md, test_size=0.1, random_state=42, stratify=Y_md\n",
    "            )\n",
    "\n",
    "            model = TabPFNClassifier.create_default_for_version(ModelVersion.V2_5)\n",
    "            model.fit(X_train_md.values, y_train_md.values)\n",
    "\n",
    "            pred = model.predict(X_test_md.values)\n",
    "            pred_proba = model.predict_proba(X_test_md.values)\n",
    "\n",
    "            f1 = f1_score(y_test_md, pred, average='weighted')\n",
    "            auc = roc_auc_score(y_test_md, pred_proba[:, 1])\n",
    "\n",
    "            print(f\"  N_feat: {X_train_md.shape[1]:3d} (MD 전체) - F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            saved_models['md_only_all'] = {\n",
    "                'model': model,\n",
    "                'method': method,\n",
    "                'n_features': X_train_md.shape[1],\n",
    "                'selected_features': list(X_train_md.columns),\n",
    "                'transformer': None,\n",
    "                'X_test': X_test_md,\n",
    "                'y_test': y_test_md\n",
    "            }\n",
    "\n",
    "            results.append({\n",
    "                'method': method,\n",
    "                'n_features': X_train_md.shape[1],\n",
    "                'f1': f1,\n",
    "                'auc': auc,\n",
    "                'model_key': 'md_only_all'\n",
    "            })\n",
    "\n",
    "        # ── All features ──────────────────────────────────────────\n",
    "        elif method == 'all':\n",
    "            X_train_sel = X_train.copy()\n",
    "            X_test_sel = X_test.copy()\n",
    "\n",
    "            model = TabPFNClassifier.create_default_for_version(ModelVersion.V2_5)\n",
    "            model.fit(X_train_sel.values, y_train.values)\n",
    "\n",
    "            pred = model.predict(X_test_sel.values)\n",
    "            pred_proba = model.predict_proba(X_test_sel.values)\n",
    "\n",
    "            f1 = f1_score(y_test, pred, average='weighted')\n",
    "            auc = roc_auc_score(y_test, pred_proba[:, 1])\n",
    "\n",
    "            print(f\"  N_feat: {X_train.shape[1]:3d} (전체) - F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "            saved_models['all_all'] = {\n",
    "                'model': model,\n",
    "                'method': method,\n",
    "                'n_features': X_train.shape[1],\n",
    "                'selected_features': list(X_train.columns),\n",
    "                'transformer': None\n",
    "            }\n",
    "\n",
    "            results.append({\n",
    "                'method': method,\n",
    "                'n_features': X_train.shape[1],\n",
    "                'f1': f1,\n",
    "                'auc': auc,\n",
    "                'model_key': 'all_all'\n",
    "            })\n",
    "\n",
    "        # ── Feature selection methods ─────────────────────────────\n",
    "        else:\n",
    "            for n_feat in n_features_list:\n",
    "                transformer = None\n",
    "                features = None\n",
    "\n",
    "                if method == 'random':\n",
    "                    np.random.seed(42)\n",
    "                    idx = np.random.choice(X_train.shape[1], n_feat, replace=False)\n",
    "                    features = X_train.columns[idx].tolist()\n",
    "                    X_train_sel = X_train[features]\n",
    "                    X_test_sel = X_test[features]\n",
    "\n",
    "                elif method == 'mi':\n",
    "                    mi_scores = mutual_info_classif(X_train, y_train, random_state=42, n_jobs=-1)\n",
    "                    idx = np.argsort(mi_scores)[-n_feat:]\n",
    "                    features = X_train.columns[idx].tolist()\n",
    "                    X_train_sel = X_train[features]\n",
    "                    X_test_sel = X_test[features]\n",
    "\n",
    "                elif method == 'pca':\n",
    "                    pca = PCA(n_components=n_feat, random_state=42)\n",
    "                    X_train_sel = pca.fit_transform(X_train)\n",
    "                    X_test_sel = pca.transform(X_test)\n",
    "                    transformer = pca\n",
    "\n",
    "                elif method == 'rfe':\n",
    "                    rf_base = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)\n",
    "                    rfe = RFE(estimator=rf_base, n_features_to_select=n_feat, step=50, verbose=0)\n",
    "                    rfe.fit(X_train, y_train)\n",
    "                    features = X_train.columns[rfe.support_].tolist()\n",
    "                    X_train_sel = X_train[features]\n",
    "                    X_test_sel = X_test[features]\n",
    "                    transformer = rfe\n",
    "\n",
    "                elif method == 'shap':\n",
    "                    rf_shap = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "                    rf_shap.fit(X_train, y_train)\n",
    "\n",
    "                    sample_size = min(1000, X_train.shape[0])\n",
    "                    X_sample = X_train.sample(n=sample_size, random_state=42)\n",
    "\n",
    "                    explainer = shap.TreeExplainer(rf_shap)\n",
    "                    shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "                    shap_importance = np.abs(shap_values).mean(axis=0).mean(axis=1)\n",
    "                    idx = np.argsort(shap_importance)[-n_feat:]\n",
    "                    features = X_train.columns[idx].tolist()\n",
    "                    X_train_sel = X_train[features]\n",
    "                    X_test_sel = X_test[features]\n",
    "\n",
    "                model = TabPFNClassifier.create_default_for_version(ModelVersion.V2_5)\n",
    "                model.fit(X_train_sel, y_train.values)\n",
    "\n",
    "                pred = model.predict(X_test_sel)\n",
    "                pred_proba = model.predict_proba(X_test_sel)\n",
    "\n",
    "                f1 = f1_score(y_test, pred, average='weighted')\n",
    "                auc = roc_auc_score(y_test, pred_proba[:, 1])\n",
    "\n",
    "                model_key = f'{method}_{n_feat}'\n",
    "                saved_models[model_key] = {\n",
    "                    'model': model,\n",
    "                    'method': method,\n",
    "                    'n_features': n_feat,\n",
    "                    'selected_features': features,\n",
    "                    'transformer': transformer\n",
    "                }\n",
    "\n",
    "                results.append({\n",
    "                    'method': method,\n",
    "                    'n_features': n_feat,\n",
    "                    'f1': f1,\n",
    "                    'auc': auc,\n",
    "                    'model_key': model_key\n",
    "                })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    for model_key, model_info in saved_models.items():\n",
    "        save_path = os.path.join(model_save_dir, f'{model_key}.joblib')\n",
    "        joblib.dump(model_info, save_path)\n",
    "    print(f\"\\n모델 {len(saved_models)}개 저장 완료: {model_save_dir}\")\n",
    "\n",
    "    results_df.sort_values('auc', ascending=False).to_csv(\n",
    "        os.path.join(model_save_dir, 'results.csv'), index=False)\n",
    "\n",
    "else:\n",
    "    for f in sorted(os.listdir(model_save_dir)):\n",
    "        if f.endswith('.joblib'):\n",
    "            model_key = f.replace('.joblib', '')\n",
    "            saved_models[model_key] = joblib.load(os.path.join(model_save_dir, f))\n",
    "\n",
    "    results_df = pd.read_csv(os.path.join(model_save_dir, 'results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98497b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>n_features</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "      <th>model_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pca</td>\n",
       "      <td>50</td>\n",
       "      <td>0.975879</td>\n",
       "      <td>0.989539</td>\n",
       "      <td>pca_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pca</td>\n",
       "      <td>500</td>\n",
       "      <td>0.972714</td>\n",
       "      <td>0.985106</td>\n",
       "      <td>pca_500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shap</td>\n",
       "      <td>200</td>\n",
       "      <td>0.972158</td>\n",
       "      <td>0.990603</td>\n",
       "      <td>shap_200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>random</td>\n",
       "      <td>300</td>\n",
       "      <td>0.971544</td>\n",
       "      <td>0.962589</td>\n",
       "      <td>random_300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pca</td>\n",
       "      <td>300</td>\n",
       "      <td>0.968506</td>\n",
       "      <td>0.984752</td>\n",
       "      <td>pca_300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pca</td>\n",
       "      <td>400</td>\n",
       "      <td>0.968506</td>\n",
       "      <td>0.986525</td>\n",
       "      <td>pca_400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rfe</td>\n",
       "      <td>400</td>\n",
       "      <td>0.968506</td>\n",
       "      <td>0.981915</td>\n",
       "      <td>rfe_400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pca</td>\n",
       "      <td>200</td>\n",
       "      <td>0.968506</td>\n",
       "      <td>0.986702</td>\n",
       "      <td>pca_200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rfe</td>\n",
       "      <td>200</td>\n",
       "      <td>0.968506</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>rfe_200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>shap</td>\n",
       "      <td>400</td>\n",
       "      <td>0.968506</td>\n",
       "      <td>0.976596</td>\n",
       "      <td>shap_400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mi</td>\n",
       "      <td>500</td>\n",
       "      <td>0.968506</td>\n",
       "      <td>0.975355</td>\n",
       "      <td>mi_500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shap</td>\n",
       "      <td>300</td>\n",
       "      <td>0.968506</td>\n",
       "      <td>0.984574</td>\n",
       "      <td>shap_300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mi</td>\n",
       "      <td>400</td>\n",
       "      <td>0.968506</td>\n",
       "      <td>0.970390</td>\n",
       "      <td>mi_400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>rfe</td>\n",
       "      <td>100</td>\n",
       "      <td>0.967838</td>\n",
       "      <td>0.952482</td>\n",
       "      <td>rfe_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>random</td>\n",
       "      <td>500</td>\n",
       "      <td>0.967100</td>\n",
       "      <td>0.970035</td>\n",
       "      <td>random_500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>random</td>\n",
       "      <td>400</td>\n",
       "      <td>0.967100</td>\n",
       "      <td>0.961525</td>\n",
       "      <td>random_400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>random</td>\n",
       "      <td>200</td>\n",
       "      <td>0.967100</td>\n",
       "      <td>0.954433</td>\n",
       "      <td>random_200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>md_only</td>\n",
       "      <td>16</td>\n",
       "      <td>0.967100</td>\n",
       "      <td>0.956028</td>\n",
       "      <td>md_only_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>shap</td>\n",
       "      <td>500</td>\n",
       "      <td>0.964918</td>\n",
       "      <td>0.984043</td>\n",
       "      <td>shap_500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>all</td>\n",
       "      <td>1040</td>\n",
       "      <td>0.964203</td>\n",
       "      <td>0.976064</td>\n",
       "      <td>all_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pca</td>\n",
       "      <td>100</td>\n",
       "      <td>0.964203</td>\n",
       "      <td>0.992730</td>\n",
       "      <td>pca_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>shap</td>\n",
       "      <td>100</td>\n",
       "      <td>0.963414</td>\n",
       "      <td>0.968794</td>\n",
       "      <td>shap_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mi</td>\n",
       "      <td>200</td>\n",
       "      <td>0.963414</td>\n",
       "      <td>0.972518</td>\n",
       "      <td>mi_200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mi</td>\n",
       "      <td>100</td>\n",
       "      <td>0.963414</td>\n",
       "      <td>0.972163</td>\n",
       "      <td>mi_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rfe</td>\n",
       "      <td>500</td>\n",
       "      <td>0.960632</td>\n",
       "      <td>0.981560</td>\n",
       "      <td>rfe_500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rfe</td>\n",
       "      <td>300</td>\n",
       "      <td>0.960632</td>\n",
       "      <td>0.985993</td>\n",
       "      <td>rfe_300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mi</td>\n",
       "      <td>50</td>\n",
       "      <td>0.959798</td>\n",
       "      <td>0.971277</td>\n",
       "      <td>mi_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mi</td>\n",
       "      <td>300</td>\n",
       "      <td>0.959798</td>\n",
       "      <td>0.967730</td>\n",
       "      <td>mi_300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>shap</td>\n",
       "      <td>50</td>\n",
       "      <td>0.959798</td>\n",
       "      <td>0.967376</td>\n",
       "      <td>shap_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>rfe</td>\n",
       "      <td>50</td>\n",
       "      <td>0.959798</td>\n",
       "      <td>0.967376</td>\n",
       "      <td>rfe_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>random</td>\n",
       "      <td>100</td>\n",
       "      <td>0.945890</td>\n",
       "      <td>0.920035</td>\n",
       "      <td>random_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>random</td>\n",
       "      <td>50</td>\n",
       "      <td>0.926399</td>\n",
       "      <td>0.794149</td>\n",
       "      <td>random_50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     method  n_features        f1       auc    model_key\n",
       "2       pca          50  0.975879  0.989539       pca_50\n",
       "6       pca         500  0.972714  0.985106      pca_500\n",
       "1      shap         200  0.972158  0.990603     shap_200\n",
       "25   random         300  0.971544  0.962589   random_300\n",
       "7       pca         300  0.968506  0.984752      pca_300\n",
       "4       pca         400  0.968506  0.986525      pca_400\n",
       "10      rfe         400  0.968506  0.981915      rfe_400\n",
       "3       pca         200  0.968506  0.986702      pca_200\n",
       "15      rfe         200  0.968506  0.975000      rfe_200\n",
       "12     shap         400  0.968506  0.976596     shap_400\n",
       "14       mi         500  0.968506  0.975355       mi_500\n",
       "8      shap         300  0.968506  0.984574     shap_300\n",
       "19       mi         400  0.968506  0.970390       mi_400\n",
       "29      rfe         100  0.967838  0.952482      rfe_100\n",
       "20   random         500  0.967100  0.970035   random_500\n",
       "26   random         400  0.967100  0.961525   random_400\n",
       "28   random         200  0.967100  0.954433   random_200\n",
       "27  md_only          16  0.967100  0.956028  md_only_all\n",
       "9      shap         500  0.964918  0.984043     shap_500\n",
       "13      all        1040  0.964203  0.976064      all_all\n",
       "0       pca         100  0.964203  0.992730      pca_100\n",
       "21     shap         100  0.963414  0.968794     shap_100\n",
       "16       mi         200  0.963414  0.972518       mi_200\n",
       "17       mi         100  0.963414  0.972163       mi_100\n",
       "11      rfe         500  0.960632  0.981560      rfe_500\n",
       "5       rfe         300  0.960632  0.985993      rfe_300\n",
       "18       mi          50  0.959798  0.971277        mi_50\n",
       "22       mi         300  0.959798  0.967730       mi_300\n",
       "23     shap          50  0.959798  0.967376      shap_50\n",
       "24      rfe          50  0.959798  0.967376       rfe_50\n",
       "30   random         100  0.945890  0.920035   random_100\n",
       "31   random          50  0.926399  0.794149    random_50"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values('f1',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c069fac4",
   "metadata": {},
   "source": [
    "## Best model SHAP 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3de4fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pca_50] F1: 0.9759, AUC: 0.9895\n",
      "Background: 50개, 샘플: 259개\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14b51ce66644c3383966a12efb360d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shap_200] F1: 0.9722, AUC: 0.9906\n",
      "Background: 50개, 샘플: 259개\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac7dd5a5ec34cab901b09a61445311e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[md_only_all] F1: 0.9671, AUC: 0.9560\n",
      "Background: 50개, 샘플: 259개\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba30f0929a44d17a3e5b376b204dccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[all_all] F1: 0.9642, AUC: 0.9761\n",
      "Background: 50개, 샘플: 259개\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d119f9271bc418d9ba4a8a28de20535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for best_model_key in ['pca_50', 'shap_200', 'md_only_all','all_all']:\n",
    "\n",
    "    # ── 모델 로드 ──────────────────────────────────────────────\n",
    "    shap_dir = os.path.join(result_dir, 'SHAP')\n",
    "    os.makedirs(shap_dir, exist_ok=True)\n",
    "    model_info = joblib.load(os.path.join(model_save_dir, f'{best_model_key}.joblib'))\n",
    "\n",
    "    model = model_info['model']\n",
    "    method = model_info['method']\n",
    "    transformer = model_info['transformer']\n",
    "    selected_features = model_info['selected_features']\n",
    "\n",
    "    # ── 데이터/feature 준비 ────────────────────────────────────\n",
    "    if method == 'md_only':\n",
    "        X_tr = model_info['X_test'].values  # md_only는 별도 저장\n",
    "        X_te = model_info['X_test'].values\n",
    "        y_te = model_info['y_test']\n",
    "        feature_names = model_info['selected_features']\n",
    "    elif method == 'pca':\n",
    "        X_tr = transformer.transform(X_train)\n",
    "        X_te = transformer.transform(X_test)\n",
    "        y_te = y_test\n",
    "        n_feat = model_info['n_features']\n",
    "        feature_names = [f'PC{i+1}' for i in range(n_feat)]\n",
    "    elif method == 'rfe':\n",
    "        X_tr = X_train[selected_features].values\n",
    "        X_te = X_test[selected_features].values\n",
    "        y_te = y_test\n",
    "        feature_names = selected_features\n",
    "    else:  # all, random, mi, shap\n",
    "        X_tr = X_train[selected_features].values if selected_features else X_train.values\n",
    "        X_te = X_test[selected_features].values if selected_features else X_test.values\n",
    "        y_te = y_test\n",
    "        feature_names = selected_features if selected_features else X_train.columns.tolist()\n",
    "\n",
    "    # ── 성능 확인 ──────────────────────────────────────────────\n",
    "    pred = model.predict(X_te)\n",
    "    pred_proba = model.predict_proba(X_te)\n",
    "    f1 = f1_score(y_te, pred, average='weighted')\n",
    "    auc = roc_auc_score(y_te, pred_proba[:, 1])\n",
    "    print(f\"[{best_model_key}] F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "    # ── SHAP 계산 ──────────────────────────────────────────────\n",
    "    def model_predict(X):\n",
    "        return model.predict_proba(X)\n",
    "\n",
    "    X_background = X_tr[:min(50, X_tr.shape[0])]\n",
    "    print(f\"Background: {len(X_background)}개, 샘플: {X_te.shape[0]}개\")\n",
    "\n",
    "    explainer = shap.KernelExplainer(model_predict, X_background)\n",
    "    shap_values = explainer.shap_values(X_te, nsamples=100)\n",
    "\n",
    "    sv = np.array(shap_values)\n",
    "    if sv.ndim == 3:\n",
    "        shap_values_class1 = sv[:, :, 1]\n",
    "    else:\n",
    "        shap_values_class1 = sv[1]\n",
    "\n",
    "    # ── 시각화 ────────────────────────────────────────────────\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values_class1, X_te, feature_names=feature_names, show=False, max_display=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{shap_dir}/shap_{best_model_key}_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_class1, X_te, feature_names=feature_names, plot_type='bar', show=False, max_display=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{shap_dir}/shap_{best_model_key}_bar.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # ── Feature Importance 저장 ────────────────────────────────\n",
    "    pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'mean_abs_shap': np.abs(shap_values_class1).mean(axis=0)\n",
    "    }).sort_values('mean_abs_shap', ascending=False).to_csv(\n",
    "        f'{shap_dir}/shap_{best_model_key}_importance.csv', index=False)\n",
    "\n",
    "    # ── PCA일 경우 components 추가 저장 ───────────────────────\n",
    "    if method == 'pca':\n",
    "        pd.DataFrame(\n",
    "            transformer.components_,\n",
    "            columns=X_train.columns,\n",
    "            index=feature_names\n",
    "        ).to_csv(f'{shap_dir}/pca_components_{best_model_key}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2e612",
   "metadata": {},
   "source": [
    "## 생성된 모델들로 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54aab094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model_batched(model_key, new_data, training_columns, batch_size=500, verbose=False):\n",
    "    model_info = saved_models[model_key]\n",
    "    model = model_info['model']\n",
    "    method = model_info['method']\n",
    "    selected_features = model_info['selected_features']\n",
    "    transformer = model_info['transformer']\n",
    "    \n",
    "    new_data_features = new_data[training_columns]\n",
    "    \n",
    "    n_samples = len(new_data_features)\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    all_preds = []\n",
    "    all_probas = []\n",
    "    \n",
    "    # 배치별 예측\n",
    "    batch_iter = range(n_batches)\n",
    "    if verbose:\n",
    "        batch_iter = tqdm(batch_iter, desc=f\"  {model_key}\", leave=False)\n",
    "    \n",
    "    for i in batch_iter:\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, n_samples)\n",
    "        batch_data = new_data_features.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Feature 변환\n",
    "        if method == 'pca':\n",
    "            batch_transformed = transformer.transform(batch_data)\n",
    "        elif method == 'rfe':\n",
    "            batch_transformed = batch_data[selected_features].values\n",
    "        else:\n",
    "            batch_transformed = batch_data[selected_features].values\n",
    "        \n",
    "        # 예측\n",
    "        pred = model.predict(batch_transformed)\n",
    "        pred_proba = model.predict_proba(batch_transformed)\n",
    "        \n",
    "        all_preds.append(pred)\n",
    "        all_probas.append(pred_proba)\n",
    "    \n",
    "    # 결과 합치기\n",
    "    final_pred = np.concatenate(all_preds)\n",
    "    final_proba = np.vstack(all_probas)\n",
    "    \n",
    "    predictions = pd.DataFrame({\n",
    "        'prediction': final_pred,\n",
    "        'probability_class_0': final_proba[:, 0],\n",
    "        'probability_class_1': final_proba[:, 1]\n",
    "    })\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "978a0690",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = '10x'\n",
    "foodb_df = pd.read_csv(f\"../data/foodb/filtered_foodb_{ratio}.csv\")\n",
    "\n",
    "training_columns = X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "output_dir = '../result/FTO_TABPFN'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "existing_files = set()\n",
    "for f in os.listdir(output_dir):\n",
    "    if f.endswith('_foodb_predictions.csv'):\n",
    "        model_key = f.replace('_foodb_predictions.csv', '')\n",
    "        existing_files.add(model_key)\n",
    "\n",
    "models_to_predict = {k: v for k, v in saved_models.items() if k not in existing_files}\n",
    "all_sheets = {}\n",
    "total_start = time.time()\n",
    "\n",
    "for model_key in tqdm(models_to_predict.keys(), desc=\"전체 진행\"):\n",
    "\n",
    "    predictions = predict_with_model_batched(\n",
    "        model_key, foodb_df, training_columns,\n",
    "        batch_size=BATCH_SIZE, verbose=False\n",
    "    )\n",
    "\n",
    "    class1_mask = predictions['prediction'] == 1\n",
    "\n",
    "    if class1_mask.sum() > 0:\n",
    "        result_df = pd.DataFrame({\n",
    "            'id': foodb_df.loc[class1_mask, 'id'].values,\n",
    "            'canonical_SMILES': foodb_df.loc[class1_mask, 'canonical_SMILES'].values,\n",
    "            'prediction': predictions.loc[class1_mask, 'prediction'].values,\n",
    "            'probability': predictions.loc[class1_mask, 'probability_class_1'].values\n",
    "        }).sort_values('probability', ascending=False).drop_duplicates('canonical_SMILES')\n",
    "    else:\n",
    "        result_df = pd.DataFrame(columns=['id', 'canonical_SMILES', 'prediction', 'probability'])\n",
    "\n",
    "    csv_path = f'{output_dir}/{model_key}_foodb_predictions.csv'\n",
    "    result_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    all_sheets[model_key] = result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = f'{output_dir}/all_foodb_predictions.xlsx'\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "    for model_key, df in all_sheets.items():\n",
    "        df.to_excel(writer, sheet_name=model_key[:31], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qsar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
